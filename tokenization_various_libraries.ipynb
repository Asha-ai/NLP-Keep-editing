{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization using \n",
    "-  1.NLTK\n",
    "-  2.spacy\n",
    "-  3.textblob\n",
    "-  4. gensim\n",
    "-  5. tensorflow\n",
    "-  6. bert\n",
    "-  7. Enchant \n",
    "-  8. Tokenization using Regular Expressions (RegEx)\n",
    "-  9. Keras - Tokenization \n",
    "-  10.Split() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Tokenization\n",
    "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens\n",
    "(or)\n",
    "Tokenization or word segmentation is a simple process of separating sentences or words from the corpus into small units, i.e. tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of tokenization\n",
    "white space tokenization, Dictionary based, Rule based, Penn Tree, spacy, Moses, \n",
    "Subword Tokenization(Byte pair, word piece, Sentence piece, unigram language model)\n",
    "### White Space Tokenization\n",
    "This is the simplest tokenization technique. Given a sentence or paragraph it tokenizes into words by splitting the input whenever a white space in encountered. \n",
    "\n",
    "### Dictionary Based Tokenization\n",
    "In this method the tokens are found based on the tokens already existing in the dictionary. If the token is not found, then special rules are used to tokenize it. It is an advanced technique compared to whitespace tokenizer.\n",
    "\n",
    "### Rule Based Tokenization\n",
    "In this technique a set of rules are created for the specific problem. The tokenization is done based on the rules. For example creating rules bases on grammar for particular language.\n",
    "\n",
    "### Regular Expression Tokenizer\n",
    "This technique uses regular expression to control the tokenization of text into tokens. Regular expression can be simple to complex and sometimes difficult to comprehend. This technique should be preferred when the above methods does not serve the required purpose. It is a rule based tokenizer.\n",
    "\n",
    "### Penn TreeBank Tokenization\n",
    "Tree bank is a corpus created which gives the semantic and syntactical annotation of language. Penn Treebank is one of the largest treebanks which was published. This technique of tokenization separates the punctuation, clitics (words that occur along with other words like I’m, don’t) and hyphenated words together.\n",
    "\n",
    "### Spacy Tokenizer\n",
    "This is a modern technique of tokenization which faster and easily customizable. It provides the flexibility to specify special tokens that need not be segmented or need to be segmented using special rules. Suppose you want to keep # as a separate token, it takes precedence over other tokenization operations.\n",
    "\n",
    "### Moses Tokenizer\n",
    "This is a tokenizer which is advanced and is available before Spacy was introduced. It is basically a collection of complex normalization and segmentation logic which works very well for structured language like English.\n",
    "\n",
    "##### Subword Tokenization\n",
    "This tokenization is very useful for specific application where sub words make significance. In this technique the most frequently used words are given unique ids and less frequent words are split into sub words and they best represent the meaning independently.\n",
    "- This helps the language model not to learn fewer and fewest as two separate words. \n",
    "Byte-Pair Encoding (BPE)\n",
    "This technique is based on the concepts in information theory and compression. BPE uses Huffman encoding for tokenization meaning it uses more embedding or symbols for representing less frequent words and less symbols or embedding for more frequently used words.\n",
    "\n",
    "##### WordPiece\n",
    "WordPiece is similar to BPE techniques expect the way the new token is added to the vocabulary. BPE considers the token with most frequent occurring pair of symbols to merge into the vocabulary. While WordPiece considers the frequency of individual symbols also and based on below count it merges into the vocabulary.\n",
    "Count (x, y) = frequency of (x, y) / frequency (x) * frequency (y)\n",
    "The pair of symbols with maximum count will be considered to merge into vocabulary. So it allows rare tokens to be included into vocabulary as compared to BPE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus (or corpora in plural) - is simply a certain collection of language data (e.g. texts). Corpora are normally used for training different models of text classification or sentiment analysis, for instance.\n",
    "\n",
    "Token - is a final string that is detached from the primary text, or in other words, it's an output of tokenization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLTK Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK - We recommend NLTK only as an education and research tool. Its modularized structure makes it excellent for learning and exploring NLP concepts, but it’s not meant for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "#Tokenization using nltk\n",
    "import nltk\n",
    "nltk.download()\n",
    "paragraph = '''\n",
    "The Telecommunications Industry has been among the best performing industries in the world in \n",
    "recent years. Telecom companies face a unique set of challenges that stem from technology \n",
    "trends and customer demands. The convergence of applications, networks or content in this\n",
    "new-age information super highway has become the next path-breaking move in core mass-market \n",
    "technology providing single connectivity and integrated user experience. \n",
    "'''\n",
    "# sentence tokenization\n",
    "sentences = nltk.sentences(paragraph)\n",
    "print(sentences)\n",
    "# word tokenization\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. spacy Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.0.12)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy) (0.2.9)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy) (1.16.6)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy) (1.31.2)\n",
      "Requirement already satisfied: ujson>=1.35 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy) (3.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.3 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy) (6.10.3)\n",
      "Requirement already satisfied: regex==2017.4.5 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy) (2017.4.5)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.12.0)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.6.2)\n",
      "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.7.1)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.48.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Error: Couldn't link model to 'en'\u001b[0m\n",
      "    Creating a symlink in spacy/data failed. Make sure you have the required\n",
      "    permissions and try re-running the command as admin, or use a\n",
      "    virtualenv. You can still import the model as a module and call its\n",
      "    load() method, or create the symlink manually.\n",
      "\n",
      "    C:\\Users\\asha.ponnada\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\en_core_web_sm\n",
      "    -->\n",
      "    C:\\Users\\asha.ponnada\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\data\\en\n",
      "\n",
      "\n",
      "\u001b[93m    Creating a shortcut link for 'en' didn't work (maybe you don't have\n",
      "    admin permissions?), but you can still load the model via its full\n",
      "    package name: nlp = spacy.load('{name}')\u001b[0m\n",
      "    Download successful but linking failed\n",
      "\n",
      "Word Tokenization: ['\\n', 'The', 'Telecommunications', 'Industry', 'has', 'been', 'among', 'the', 'best', 'performing', 'industries', 'in', 'the', 'world', 'in', '\\n', 'recent', 'years', '.', 'Telecom', 'companies', 'face', 'a', 'unique', 'set', 'of', 'challenges', 'that', 'stem', 'from', 'technology', '\\n', 'trends', 'and', 'customer', 'demands', '.', 'The', 'convergence', 'of', 'applications', ',', 'networks', 'or', 'content', 'in', 'this', '\\n', 'new', '-', 'age', 'information', 'super', 'highway', 'has', 'become', 'the', 'next', 'path', '-', 'breaking', 'move', 'in', 'core', 'mass', '-', 'market', '\\n', 'technology', 'providing', 'single', 'connectivity', 'and', 'integrated', 'user', 'experience', '.', '\\n']\n",
      "sentence list:  ['\\nThe Telecommunications Industry has been among the best performing industries in the world in \\nrecent years.', 'Telecom companies face a unique set of challenges that stem from technology \\ntrends and customer demands.', 'The convergence of applications, networks or content in this\\nnew-age information super highway has become the next path-breaking move in core mass-market \\ntechnology providing single connectivity and integrated user experience.', '\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\asha.ponnada\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "text = '''\n",
    "The Telecommunications Industry has been among the best performing industries in the world in \n",
    "recent years. Telecom companies face a unique set of challenges that stem from technology \n",
    "trends and customer demands. The convergence of applications, networks or content in this\n",
    "new-age information super highway has become the next path-breaking move in core mass-market \n",
    "technology providing single connectivity and integrated user experience. \n",
    "'''\n",
    "# Word Tokenization\n",
    "doc = nlp(text)\n",
    "# create list of word tokens\n",
    "token_list = []\n",
    "for t in doc:\n",
    "    token_list.append(t.text)\n",
    "print(\"Word Tokenization:\", token_list)\n",
    "\n",
    "# sentence tokenization\n",
    "\n",
    "# Load English tokenizer\n",
    "nlp = English()\n",
    "\n",
    "# Create the pipeline 'sentencizer' component\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "# create list of sentence tokens\n",
    "sents_list = []\n",
    "for sent in doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "print(\"sentence list: \",sents_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. textblob Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textblob - TextBlob is built on top of NLTK, and it’s more easily-accessible. This is our favorite library for fast-prototyping or building applications that don’t require highly optimized performance. Beginners should start here.\n",
    "\n",
    "TextBlob can processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "Noun phrase extraction\n",
    "Part-of-speech tagging\n",
    "Sentiment analysis\n",
    "Classification (Naive Bayes, Decision Tree)\n",
    "Tokenization (splitting text into words and sentences)\n",
    "Word and phrase frequencies\n",
    "Parsing\n",
    "n-grams\n",
    "Word inflection (pluralization and singularization) and lemmatization\n",
    "Spelling correction\n",
    "Add new models or languages through extensions\n",
    "WordNet integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: textblob in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied, skipping upgrade: nltk>=3.1 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from textblob) (3.2.5)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\asha.ponnada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asha.ponnada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\asha.ponnada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\asha.ponnada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\asha.ponnada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\asha.ponnada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n",
      " --------- Sentence tokenization using textblob ----------\n",
      "[Sentence(\"The Telecommunications Industry has been among the best performing industries in the world in \n",
      "recent years.\"), Sentence(\"Telecom companies face a unique set of challenges that stem from technology \n",
      "trends and customer demands.\"), Sentence(\"The convergence of applications, networks or content in this\n",
      "new-age information super highway has become the next path-breaking move in core mass-market \n",
      "technology providing single connectivity and integrated user experience.\")]\n",
      "['The', 'Telecommunications', 'Industry', 'has', 'been', 'among', 'the', 'best', 'performing', 'industries', 'in', 'the', 'world', 'in', 'recent', 'years', 'Telecom', 'companies', 'face', 'a', 'unique', 'set', 'of', 'challenges', 'that', 'stem', 'from', 'technology', 'trends', 'and', 'customer', 'demands', 'The', 'convergence', 'of', 'applications', 'networks', 'or', 'content', 'in', 'this', 'new-age', 'information', 'super', 'highway', 'has', 'become', 'the', 'next', 'path-breaking', 'move', 'in', 'core', 'mass-market', 'technology', 'providing', 'single', 'connectivity', 'and', 'integrated', 'user', 'experience']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization using TextBlog\n",
    "\n",
    "!pip install -U textblob\n",
    "!python -m textblob.download_corpora\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "corpus = '''The Telecommunications Industry has been among the best performing industries in the world in \n",
    "recent years. Telecom companies face a unique set of challenges that stem from technology \n",
    "trends and customer demands. The convergence of applications, networks or content in this\n",
    "new-age information super highway has become the next path-breaking move in core mass-market \n",
    "technology providing single connectivity and integrated user experience.\n",
    "'''\n",
    "blob_obj = TextBlob(corpus)\n",
    "\n",
    "print(\" --------- Sentence tokenization using textblob ----------\")\n",
    "print(blob_obj.sentences)\n",
    "# Word Tokenize\n",
    "print(blob_obj.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. gensim Tokenization\n",
    "gensim -Gensim is most commonly used for topic modeling and similarity detection. It’s not a general-purpose NLP library, but for the tasks it does handle, it does them well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has: 29 tokens\n",
      "{'T': 0, 'h': 1, 'e': 2, 'l': 3, 'c': 4, 'o': 5, 'm': 6, 'u': 7, 'n': 8, 'i': 9, 'a': 10, 't': 11, 's': 12, 'I': 13, 'd': 14, 'r': 15, 'y': 16, 'b': 17, 'g': 18, 'p': 19, 'f': 20, 'w': 21, '.': 22, 'q': 23, 'v': 24, ',': 25, 'k': 26, '-': 27, 'x': 28}\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "text = '''The Telecommunications Industry has been among the best performing industries in the world in \n",
    "recent years. Telecom companies face a unique set of challenges that stem from technology \n",
    "trends and customer demands. The convergence of applications, networks or content in this\n",
    "new-age information super highway has become the next path-breaking move in core mass-market \n",
    "technology providing single connectivity and integrated user experience.'''\n",
    "\n",
    "tokens = [[token for token in sentence.split()] for sentence in text]\n",
    "gensim_dictionary.add_documents(tokens)\n",
    "\n",
    "print(\"The dictionary has: \" + str(len(gensim_dictionary)) + \" tokens\")\n",
    "print(gensim_dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. tensorflow Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'where': 1, 'to': 2, 'go': 3, 'how': 4, 'will': 5, 'we': 6, 'move': 7}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "sentences = [\n",
    "    'where to go',\n",
    "    'how will we move'\n",
    "]\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. bert Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n",
      "100%|███████████████████████████████| 213450/213450 [00:00<00:00, 227192.99B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['algorithm',\n",
       " 'that',\n",
       " 'breaks',\n",
       " 'a',\n",
       " 'word',\n",
       " 'into',\n",
       " 'several',\n",
       " 'sub',\n",
       " '##words',\n",
       " ',',\n",
       " 'such',\n",
       " 'that',\n",
       " 'commonly',\n",
       " 'seen',\n",
       " 'sub',\n",
       " '##words',\n",
       " 'can']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tz.convert_tokens_to_ids([\"characteristically\"])\n",
    "sent = \"algorithm that breaks a word into several subwords, such that commonly seen subwords can\"\n",
    "tz.tokenize(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Enchant Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enchant is a module in Python which is used to check the spelling of a word, gives suggestions to correct words. Also, gives antonym and synonym of words. It checks whether a word exists in dictionary or not.\n",
    "\n",
    "Enchant is used to check the spelling of words and suggest corrections for words that are miss-spelled. It can use many popular spellchecking packages to perform this task, including ispell, aspell and MySpell. It is quite flexible at handling multiple dictionaries and multiple languages.\n",
    "\n",
    "More information is available on the Enchant website:\n",
    "\n",
    "Enchant also provides the enchant.tokenize module to tokenize text. Tokenizing involves splitting words from the body of the text.\n",
    "Ref : https://pyenchant.github.io/pyenchant/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyenchant in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.2.0)\n",
      "Help on package enchant:\n",
      "\n",
      "NAME\n",
      "    enchant\n",
      "\n",
      "DESCRIPTION\n",
      "    # pyenchant\n",
      "    #\n",
      "    # Copyright (C) 2004-2011, Ryan Kelly\n",
      "    #\n",
      "    # This library is free software; you can redistribute it and/or\n",
      "    # modify it under the terms of the GNU Lesser General Public\n",
      "    # License as published by the Free Software Foundation; either\n",
      "    # version 2.1 of the License, or (at your option) any later version.\n",
      "    #\n",
      "    # This library is distributed in the hope that it will be useful,\n",
      "    # but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
      "    # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n",
      "    # Lesser General Public License for more details.\n",
      "    #\n",
      "    # You should have received a copy of the GNU Lesser General Public\n",
      "    # License along with this library; if not, write to the\n",
      "    # Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n",
      "    # Boston, MA 02111-1307, USA.\n",
      "    #\n",
      "    # In addition, as a special exception, you are\n",
      "    # given permission to link the code of this program with\n",
      "    # non-LGPL Spelling Provider libraries (eg: a MSFT Office\n",
      "    # spell checker backend) and distribute linked combinations including\n",
      "    # the two.  You must obey the GNU Lesser General Public License in all\n",
      "    # respects for all of the code used other than said providers.  If you modify\n",
      "    # this file, you may extend this exception to your version of the\n",
      "    # file, but you are not obligated to do so.  If you do not wish to\n",
      "    # do so, delete this exception statement from your version.\n",
      "    #\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _enchant\n",
      "    checker (package)\n",
      "    errors\n",
      "    pypwl\n",
      "    tokenize (package)\n",
      "    utils\n",
      "\n",
      "FILE\n",
      "    c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\enchant\\__init__.py\n",
      "\n",
      "\n",
      "[('The', 0), ('Telecommunications', 4), ('Industry', 23), ('has', 32), ('been', 36), ('among', 41), ('the', 47), ('best', 51), ('performing', 56), ('industries', 67), ('in', 78), ('the', 81), ('world', 85), ('in', 91), ('recent', 95), ('years', 102), ('Telecom', 109), ('companies', 117), ('face', 127), ('a', 132), ('unique', 134), ('set', 141), ('of', 145), ('challenges', 148), ('that', 159), ('stem', 164), ('from', 169), ('technology', 174), ('trends', 186), ('and', 193), ('customer', 197), ('demands', 206), ('The', 215), ('convergence', 219), ('of', 231), ('applications', 234), ('networks', 248), ('or', 257), ('content', 260), ('in', 268), ('this', 271), ('new', 276), ('age', 280), ('information', 284), ('super', 296), ('highway', 302), ('has', 310), ('become', 314), ('the', 321), ('next', 325), ('path', 330), ('breaking', 335), ('move', 344), ('in', 349), ('core', 352), ('mass', 357), ('market', 362), ('technology', 370), ('providing', 381), ('single', 391), ('connectivity', 398), ('and', 411), ('integrated', 415), ('user', 426), ('experience', 431)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyenchant\n",
    "from enchant.tokenize import get_tokenizer \n",
    "import enchant\n",
    "help(enchant)\n",
    "  \n",
    "# the text to be tokenized  \n",
    "text = '''The Telecommunications Industry has been among the best performing industries in the world in \n",
    "recent years. Telecom companies face a unique set of challenges that stem from technology \n",
    "trends and customer demands. The convergence of applications, networks or content in this\n",
    "new-age information super highway has become the next path-breaking move in core mass-market \n",
    "technology providing single connectivity and integrated user experience.\n",
    "'''\n",
    "# getting tokenizer class \n",
    "tokenizer = get_tokenizer(\"en_US\") \n",
    "  \n",
    "token_list =[] \n",
    "for words in tokenizer(text): \n",
    "    token_list.append(words) \n",
    "  \n",
    "# print the words with POS \n",
    "print(token_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Telecommunications', 'Industry', 'has', 'been', 'among', 'the', 'best', 'performing', 'industries', 'in', 'the', 'world', 'in', 'recent', 'years', 'Telecom', 'companies', 'face', 'a', 'unique', 'set', 'of', 'challenges', 'that', 'stem', 'from', 'technology', 'trends', 'and', 'customer', 'demands', 'The', 'convergence', 'of', 'applications', 'networks', 'or', 'content', 'in', 'this', 'new', 'age', 'information', 'super', 'highway', 'has', 'become', 'the', 'next', 'path', 'breaking', 'move', 'in', 'core', 'mass', 'market', 'technology', 'providing', 'single', 'connectivity', 'and', 'integrated', 'user', 'experience']\n"
     ]
    }
   ],
   "source": [
    "# print only the words \n",
    "word_list =[] \n",
    "  \n",
    "for tokens in token_list: \n",
    "    word_list.append(tokens[0]) \n",
    "print(word_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Tokenization using Regular Expressions (RegEx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Word Tokenization ------ ['The', 'Telecommunications', 'Industry', 'has', 'been', 'among', 'the', 'best', 'performing', 'industries', 'in', 'the', 'world', 'in', 'recent', 'years', 'Telecom', 'companies', 'face', 'a', 'unique', 'set', 'of', 'challenges', 'that', 'stem', 'from', 'technology', 'trends', 'and', 'customer', 'demands', 'The', 'convergence', 'of', 'applications', 'networks', 'or', 'content', 'in', 'this', 'new', 'age', 'information', 'super', 'highway', 'has', 'become', 'the', 'next', 'path', 'breaking', 'move', 'in', 'core', 'mass', 'market', 'technology', 'providing', 'single', 'connectivity', 'and', 'integrated', 'user', 'experience']\n",
      "------- sent tokenization ------ ['\\nThe Telecommunications Industry has been among the best performing industries in the world in \\nrecent years', ' Telecom companies face a unique set of challenges that stem from technology \\ntrends and customer demands', ' The convergence of applications, networks or content in this\\nnew-age information super highway has become the next path-breaking move in core mass-market \\ntechnology providing single connectivity and integrated user experience', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = '''\n",
    "The Telecommunications Industry has been among the best performing industries in the world in \n",
    "recent years. Telecom companies face a unique set of challenges that stem from technology \n",
    "trends and customer demands. The convergence of applications, networks or content in this\n",
    "new-age information super highway has become the next path-breaking move in core mass-market \n",
    "technology providing single connectivity and integrated user experience.'''\n",
    "tokens = re.findall(\"[\\w']+\",text)\n",
    "print(\"------- Word Tokenization ------\",tokens)\n",
    "\n",
    "sentences = re.compile('[.!?]').split(text)\n",
    "print(\"------- sent tokenization ------\",sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Keras - Tokenization \n",
    "it does 3 things\n",
    "Splits words by space (split=” “).\n",
    "\n",
    "Filters out punctuation (filters=’!”#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n’).\n",
    "\n",
    "Converts text to lowercase (lower=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras) (1.16.6)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras) (1.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras) (1.12.0)\n",
      "word tokenization ['the', 'telecommunications', 'industry', 'has', 'been', 'among', 'the', 'best', 'performing', 'industries', 'in', 'the', 'world', 'in', 'recent', 'years', 'telecom', 'companies', 'face', 'a', 'unique', 'set', 'of', 'challenges', 'that', 'stem', 'from', 'technology', 'trends', 'and', 'customer', 'demands', 'the', 'convergence', 'of', 'applications', 'networks', 'or', 'content', 'in', 'this', 'new', 'age', 'information', 'super', 'highway', 'has', 'become', 'the', 'next', 'path', 'breaking', 'move', 'in', 'core', 'mass', 'market', 'technology', 'providing', 'single', 'connectivity', 'and', 'integrated', 'user', 'experience']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\asha.ponnada\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "result = text_to_word_sequence(text)\n",
    "print(\"word tokenization\",result)\n",
    "\n",
    "from keras.preprocessing.text import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization using Split() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Telecommunications', 'Industry', 'has', 'been', 'among', 'the', 'best', 'performing', 'industries', 'in', 'the', 'world', 'in', 'recent', 'years.', 'Telecom', 'companies', 'face', 'a', 'unique', 'set', 'of', 'challenges', 'that', 'stem', 'from', 'technology', 'trends', 'and', 'customer', 'demands.', 'The', 'convergence', 'of', 'applications,', 'networks', 'or', 'content', 'in', 'this', 'new-age', 'information', 'super', 'highway', 'has', 'become', 'the', 'next', 'path-breaking', 'move', 'in', 'core', 'mass-market', 'technology', 'providing', 'single', 'connectivity', 'and', 'integrated', 'user', 'experience.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nThe Telecommunications Industry has been among the best performing industries in the world in \\nrecent years',\n",
       " 'Telecom companies face a unique set of challenges that stem from technology \\ntrends and customer demands',\n",
       " 'The convergence of applications, networks or content in this\\nnew-age information super highway has become the next path-breaking move in core mass-market \\ntechnology providing single connectivity and integrated user experience.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tikenization using python function\n",
    "print(text.split())\n",
    "# Sent Tokenization\n",
    "text.split('. ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
