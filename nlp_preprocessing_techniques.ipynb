{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms.', 'The bag-of-words model is simple to understand and implement and has seen great success in problems such as language modeling and document classification.']\n",
      "['The', 'bag-of-words', 'model', 'is', 'a', 'way', 'of', 'representing', 'text', 'data', 'when', 'modeling', 'text', 'with', 'machine', 'learning', 'algorithms', '.', 'The', 'bag-of-words', 'model', 'is', 'simple', 'to', 'understand', 'and', 'implement', 'and', 'has', 'seen', 'great', 'success', 'in', 'problems', 'such', 'as', 'language', 'modeling', 'and', 'document', 'classification', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#nltk.download()\n",
    "p = '''The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. The bag-of-words model is simple to understand and implement and has seen great success in problems such as language modeling and document classification.\n",
    "'''\n",
    "# tokenizing sentences\n",
    "sen = nltk.sent_tokenize(p)\n",
    "print(sen)\n",
    "words = nltk.word_tokenize(p)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”. Stemming is an important part of the pipelining process in Natural language processing. The input to the stemmer is tokenized words. How do we get these tokenized words? Well, tokenization involves breaking down the document into different words.\n",
    "\n",
    "Stemming is used in information retrieval systems like search engines.\n",
    "It is used to determine domain vocabularies in domain analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of stemmers\n",
    "* Porter Stemmer, \n",
    "* Lancaster Stemmer, \n",
    "* Regexp Stemmer, \n",
    "* Snowball Stemmer - SnowballStemmer class\n",
    "NLTK has SnowballStemmer class with the help of which we can easily implement Snowball Stemmer algorithms. It supports 15 non-English languages. In order to use this steaming class, we need to create an instance with the name of the language we are using and then call the stem() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming through Porter stemmer: writ\n",
      "stemming through LancasterStemmer: writ\n",
      "stemming through RegexpStemmer: writt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x2a274ca9700>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "print(\"stemming through Porter stemmer:\",word_stemmer.stem('writting'))\n",
    "\n",
    "# Lancasetr stemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "lanc_stemmer = LancasterStemmer()\n",
    "print(\"stemming through LancasterStemmer:\",lanc_stemmer.stem('writing'))\n",
    "\n",
    "# Regular exp stem -create an instance of RegexpStemmer class and provides the suffix or prefix you want to remove from the word as follows\n",
    "from nltk.stem import RegexpStemmer\n",
    "Reg_stem = RegexpStemmer('ing')\n",
    "print(\"stemming through RegexpStemmer:\",Reg_stem.stem('writting'))\n",
    "\n",
    "# snowball stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages\n",
    "eng_stemmer = SnowballStemmer('english')\n",
    "eng_stemmer                                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the bag-of-word model simpl understand implement seen great success problem languag model document classif .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "p = '''The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. The bag-of-words model is simple to understand and implement and has seen great success in problems such as language modeling and document classification.\n",
    "'''\n",
    "#\n",
    "sentences = nltk.sent_tokenize(p)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)  \n",
    "sentences[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "Lemmatization is a linguistic term that means grouping together words with the same root or lemma but with different inflections or derivatives of meaning so they can be analyzed as one item. The aim is to take away inflectional suffixes and prefixes to bring out the word's dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The bag-of-words model simple understand implement seen great success problem language modeling document classification .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "p = '''\n",
    "The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. \n",
    "The bag-of-words model is simple to understand and implement and has seen great success in problems such as\n",
    "language modeling and document classification.\n",
    "'''\n",
    "sentences = nltk.sent_tokenize(p)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)   \n",
    "sentences[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of both programs tells the major difference between stemming and lemmatization. PorterStemmer class chops off the \n",
    "‘es’ from the word. On the other hand, WordNetLemmatizer class finds a valid word. In simple words, stemming technique only\n",
    "looks at the form of the word whereas lemmatization technique looks at the meaning of the word. It means after applying \n",
    "lemmatization, we will always get a valid word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval. In this model, a text is represented as the bag of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 2, 0, 1, 0, 0, 0, 2, 0, 1, 1],\n",
       "       [0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 2, 1, 0, 1, 1, 1, 0, 1, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "paragraph = '''\n",
    "The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. The bag-of-words model is simple to understand and implement and has seen great success in problems such as language modeling and document classification.\n",
    "'''\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Cleaning the texts\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: ['The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms.', 'The bag-of-words model is simple to understand and implement and has seen great success in problems such as language\\nmodeling and document classification.']\n",
      "\n",
      "\n",
      "['bag', 'word', 'model', 'way', 'representing', 'text', 'data', 'modeling', 'text', 'machine', 'learning', 'algorithm'] ['bag', 'word', 'model', 'way', 'representing', 'text', 'data', 'modeling', 'text', 'machine', 'learning', 'algorithm']\n",
      "review1 bag word model way representing text data modeling text machine learning algorithm\n",
      "\n",
      "\n",
      "corpus: ['bag word model way representing text data modeling text machine learning algorithm']\n",
      "\n",
      "\n",
      "['bag', 'word', 'model', 'simple', 'understand', 'implement', 'seen', 'great', 'success', 'problem', 'language', 'modeling', 'document', 'classification'] ['bag', 'word', 'model', 'simple', 'understand', 'implement', 'seen', 'great', 'success', 'problem', 'language', 'modeling', 'document', 'classification']\n",
      "review1 bag word model simple understand implement seen great success problem language modeling document classification\n",
      "\n",
      "\n",
      "corpus: ['bag word model way representing text data modeling text machine learning algorithm', 'bag word model simple understand implement seen great success problem language modeling document classification']\n",
      "\n",
      "\n",
      "[[0.28837519 0.20518132 0.         0.28837519 0.         0.\n",
      "  0.         0.         0.28837519 0.28837519 0.20518132 0.20518132\n",
      "  0.         0.28837519 0.         0.         0.         0.57675038\n",
      "  0.         0.28837519 0.20518132]\n",
      " [0.         0.20518132 0.28837519 0.         0.28837519 0.28837519\n",
      "  0.28837519 0.28837519 0.         0.         0.20518132 0.20518132\n",
      "  0.28837519 0.         0.28837519 0.28837519 0.28837519 0.\n",
      "  0.28837519 0.         0.20518132]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "p='''The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. \n",
    "The bag-of-words model is simple to understand and implement and has seen great success in problems such as language\n",
    "modeling and document classification.\n",
    "'''\n",
    "# # Cleaning the texts\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(p)\n",
    "print(\"sentences:\",sentences)\n",
    "print('\\n')\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    print(review, review)\n",
    "    review1 = ' '.join(review)\n",
    "    print(\"review1\",review1)\n",
    "    print(\"\\n\")\n",
    "    corpus.append(review1)\n",
    "    print(\"corpus:\",corpus)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# create TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.\n",
    "\n",
    "Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "What are word embeddings exactly? Loosely speaking, they are vector representations of a particular word. Having said this, what follows is how do we generate them? More importantly, how do they capture the context?\n",
    "Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: ['I have three visions for India.', 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.', 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.', 'Yet we have not done this to any other nation.', 'We have not conquered anyone.', 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.', 'Why?', 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.', 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.', 'It is this freedom that\\n               we must protect and nurture and build on.', 'If we are not free, no one will respect us.', 'My second vision for India’s development.', 'For fifty years we have been a developing nation.', 'It is time we see ourselves as a developed nation.', 'We are among the top 5 nations of the world\\n               in terms of GDP.', 'We have a 10 percent growth rate in most areas.', 'Our poverty levels are falling.', 'Our achievements are being globally recognised today.', 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.', 'Isn’t this incorrect?', 'I have a third vision.', 'India must stand up to the world.', 'Because I believe that unless India \\n               stands up to the world, no one will respect us.', 'Only strength respects strength.', 'We must be \\n               strong not only as a military power but also as an economic power.', 'Both must go hand-in-hand.', 'My good fortune was to have worked with three great minds.', 'Dr. Vikram Sarabhai of the Dept.', 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.', 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.', 'I see four milestones in my career']\n",
      "\n",
      "\n",
      "stemmer: <PorterStemmer>\n",
      "words ['I', 'three', 'vision', 'india', '.']\n",
      "words ['In', '3000', 'year', 'histori', ',', 'peopl', 'world', 'come', 'invad', 'us', ',', 'captur', 'land', ',', 'conquer', 'mind', '.']\n",
      "words ['from', 'alexand', 'onward', ',', 'greek', ',', 'turk', ',', 'mogul', ',', 'portugues', ',', 'british', ',', 'french', ',', 'dutch', ',', 'came', 'loot', 'us', ',', 'took', '.']\n",
      "words ['yet', 'done', 'nation', '.']\n",
      "words ['We', 'conquer', 'anyon', '.']\n",
      "words ['We', 'grab', 'land', ',', 'cultur', ',', 'histori', 'tri', 'enforc', 'way', 'life', '.']\n",
      "words ['whi', '?']\n",
      "words ['becaus', 'respect', 'freedom', 'others.that', 'first', 'vision', 'freedom', '.']\n",
      "words ['I', 'believ', 'india', 'got', 'first', 'vision', '1857', ',', 'start', 'war', 'independ', '.']\n",
      "words ['It', 'freedom', 'must', 'protect', 'nurtur', 'build', '.']\n",
      "words ['If', 'free', ',', 'one', 'respect', 'us', '.']\n",
      "words ['My', 'second', 'vision', 'india', '’', 'develop', '.']\n",
      "words ['for', 'fifti', 'year', 'develop', 'nation', '.']\n",
      "words ['It', 'time', 'see', 'develop', 'nation', '.']\n",
      "words ['We', 'among', 'top', '5', 'nation', 'world', 'term', 'gdp', '.']\n",
      "words ['We', '10', 'percent', 'growth', 'rate', 'area', '.']\n",
      "words ['our', 'poverti', 'level', 'fall', '.']\n",
      "words ['our', 'achiev', 'global', 'recognis', 'today', '.']\n",
      "words ['yet', 'lack', 'self-confid', 'see', 'develop', 'nation', ',', 'self-reli', 'self-assur', '.']\n",
      "words ['isn', '’', 'incorrect', '?']\n",
      "words ['I', 'third', 'vision', '.']\n",
      "words ['india', 'must', 'stand', 'world', '.']\n",
      "words ['becaus', 'I', 'believ', 'unless', 'india', 'stand', 'world', ',', 'one', 'respect', 'us', '.']\n",
      "words ['onli', 'strength', 'respect', 'strength', '.']\n",
      "words ['We', 'must', 'strong', 'militari', 'power', 'also', 'econom', 'power', '.']\n",
      "words ['both', 'must', 'go', 'hand-in-hand', '.']\n",
      "words ['My', 'good', 'fortun', 'work', 'three', 'great', 'mind', '.']\n",
      "words ['dr.', 'vikram', 'sarabhai', 'dept', '.']\n",
      "words ['space', ',', 'professor', 'satish', 'dhawan', ',', 'succeed', 'dr.', 'brahm', 'prakash', ',', 'father', 'nuclear', 'materi', '.']\n",
      "words ['I', 'lucki', 'work', 'three', 'close', 'consid', 'great', 'opportun', 'life', '.']\n",
      "words ['I', 'see', 'four', 'mileston', 'career']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "p = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n",
    "sentences = nltk.sent_tokenize(p)\n",
    "print(\"sentences:\", sentences)\n",
    "stemmer = PorterStemmer()\n",
    "print('\\n')\n",
    "print(\"stemmer:\",stemmer)\n",
    "# stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words= [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    print(\"words\",words)\n",
    "    sentences[i] = ''.join(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
