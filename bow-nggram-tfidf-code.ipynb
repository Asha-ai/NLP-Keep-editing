{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary        allowed  and  antagonistic  are  cats  dogs  not\n",
      "document1 vector        1    1             0    1     1     1    1\n",
      "document2 vector        0    1             1    1     1     1    0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['Cats and dogs are not allowed', 'Cats and dogs are antagonistic']\n",
    "count_vect = CountVectorizer()\n",
    "X = count_vect.fit_transform(corpus)\n",
    "X\n",
    "\n",
    "import pandas as pd\n",
    " \n",
    "df = pd.DataFrame()\n",
    "df['vocabulary'] = count_vect.get_feature_names()\n",
    "df['document1 vector'] = X.toarray()[0]\n",
    "df['document2 vector'] = X.toarray()[1]\n",
    "df.set_index('vocabulary', inplace=True)\n",
    "print(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_IDF Example\n",
    "tf = (no of times words appear in a doc)/(totoal no of words in the doc)\n",
    "IDF - IDF measures of importance of word, taking into consideration the frequency of the word through out the corpus\n",
    "idf = log(total no of docs/no of docs with the word in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary       and  antagonistic       are      cats      dogs      four  \\\n",
      "sentence1   0.000000      0.000000  0.000000  0.402040  0.000000  0.528635   \n",
      "sentence2   0.490479      0.490479  0.490479  0.373022  0.373022  0.000000   \n",
      "sentence3   0.000000      0.000000  0.000000  0.000000  0.473630  0.000000   \n",
      "\n",
      "vocabulary      hate      have        he      legs  \n",
      "sentence1   0.000000  0.528635  0.000000  0.528635  \n",
      "sentence2   0.000000  0.000000  0.000000  0.000000  \n",
      "sentence3   0.622766  0.000000  0.622766  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    " \n",
    "corpus = ['Cats have four legs',\n",
    "          'Cats and dogs are antagonistic',\n",
    "          'He hate dogs']\n",
    "tfidf = TfidfVectorizer()\n",
    "vect = tfidf.fit_transform(corpus)\n",
    " \n",
    "df = pd.DataFrame()\n",
    "df['vocabulary'] = tfidf.get_feature_names()\n",
    "df['sentence1'] = vect.toarray()[0]\n",
    "df['sentence2'] = vect.toarray()[1]\n",
    "df['sentence3'] = vect.toarray()[2]\n",
    "df.set_index('vocabulary', inplace=True)\n",
    "print(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ng-gram example\n",
    "An N-Gram is a sequence of N-words in a sentence. Here, N is an integer which stands for the number of words in the sequence.\n",
    "\n",
    "For example, if we put N=1, then it is referred to as a uni-gram. If you put N=2, then it is a bi-gram. If we substitute N=3, then it is a tri-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'like')\n",
      "('like', 'dancing')\n",
      "('dancing', 'in')\n",
      "('in', 'the')\n",
      "('the', 'rain')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'I like dancing in the rain'\n",
    "ngram = ngrams(sentence.split(' '),n=2)\n",
    "for x in ngram:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word List for Document \n",
      "['and', 'arrived', 'at', 'bus', 'but', 'early', 'for', 'i', 'joe', 'late', 'looked', 'mary', 'noon', 'samantha', 'station', 'the', 'took', 'train', 'until', 'waited', 'was'] \n",
      "\n",
      "Joe waited for the train \n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "\n",
      "The train was late \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Mary and Samantha took the bus \n",
      "[1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "I looked for Mary and Samantha at the bus station \n",
      "[1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Mary and Samantha arrived at the bus station early but waited until noon for the bus \n",
      "[1. 1. 1. 2. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import statments\n",
    "import numpy\n",
    "import re\n",
    "\n",
    "'''\n",
    "Tokenize each the sentences, example\n",
    "Input : \"John likes to watch movies. Mary likes movies too\"\n",
    "Ouput : \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\",\"likes\",\"movies\",\"too\"\n",
    "'''\n",
    "def tokenize(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = word_extraction(sentence)\n",
    "        words.extend(w)\n",
    "        \n",
    "    words = sorted(list(set(words)))\n",
    "    return words\n",
    "\n",
    "def word_extraction(sentence):\n",
    "    ignore = ['a', \"the\", \"is\"]\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()\n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]\n",
    "    return cleaned_text    \n",
    "    \n",
    "def generate_bow(allsentences):    \n",
    "    vocab = tokenize(allsentences)\n",
    "    print(\"Word List for Document \\n{0} \\n\".format(vocab));\n",
    "\n",
    "    for sentence in allsentences:\n",
    "        words = word_extraction(sentence)\n",
    "        bag_vector = numpy.zeros(len(vocab))\n",
    "        for w in words:\n",
    "            for i,word in enumerate(vocab):\n",
    "                if word == w: \n",
    "                    bag_vector[i] += 1\n",
    "                    \n",
    "        print(\"{0} \\n{1}\\n\".format(sentence,numpy.array(bag_vector)))\n",
    "\n",
    "\n",
    "allsentences = [\"Joe waited for the train\", \"The train was late\", \"Mary and Samantha took the bus\", \n",
    "            \"I looked for Mary and Samantha at the bus station\", \n",
    "            \"Mary and Samantha arrived at the bus station early but waited until noon for the bus\"]\n",
    "\n",
    "\n",
    "generate_bow(allsentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
