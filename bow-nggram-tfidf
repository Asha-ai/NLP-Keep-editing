# converting raw text to numerical vectors using BOW, N_grams, Tf-Idf.
BOW is way of representing text data when modeling text with ML algs.
Good to use for language modeling and document classification, feature extraction.
BOW is representation of text
## BAG OF WORDS(BoW):
The BoW model captures the frequencies of the word occurrences in a text corpus.

Bag of words is not concerned about the order in which words appear in the text; instead, it only cares about which words appear in the text.

Let’s understand how BoW works with an example. Consider the following phrases:

Document 1: Cats and dogs are not allowed

Document 2: Cats and dogs are antagonistic

Bag of words will first create a unique list of all the words based on the two documents. If we consider the two documents, we will have seven unique words.

‘cats’, ‘and’, ‘dogs’, ‘are’, ‘not’, ‘allowed’, ‘antagonistic’

Each unique word is a feature or dimension.

Now for each document, a feature vector will be created. Each feature vector will be seven-dimensional since we have seven unique words.

Document1 vector: [1 1 1 1 1 1 0]

Document2 vector: [1 1 1 1 0 0 1]

The words present in the document are marked as 1 and the remaining as 0.

## Problems with TF-IDF and Bag of Words Approach
Before we go and actually implement the N-Grams model, let us first discuss the drawback of the bag of words and TF-IDF approaches.

In the bag of words and TF-IDF approach, words are treated individually and every single word is converted into its numeric counterpart. The context information of the word is not retained. Consider two sentences "big red machine and carpet" and "big red carpet and machine". If you use a bag of words approach, you will get the same vectors for these two sentences. However, we can clearly see that in the first sentence we are talking about a "big red machine", while the second sentence contains information about the "big red carpet". Hence, context information is very important. The N-Grams model basically helps us capture the context information.

Theory of N-Grams Model
Wikipedia defines an N-Gram as "A contiguous sequence of N items from a given sample of text or speech". Here an item can be a character, a word or a sentence and N can be any integer. When N is 2, we call the sequence a bigram. Similarly, a sequence of 3 items is called a trigram, and so on.

In order to understand N-Grams model, we first have to understand how the Markov chains work.

Connection of N-Grams with Markov Chains
A Markov chain is a sequence of states. Consider a Markov system with 2 states, X and Y. In a Markov chain, you can either stay at one state or move to the other state. In our example, our states have the following behavior:

The probability of moving from X to Y is 50% and similarly, the probability of staying at X is 50%.
Likewise, the probability of staying at Y is 50% while the possibility of moving back to X is also 50%.
This way a Markov sequence can be generated, such as XXYX, etc.

In an N-Grams model, an item in a sequence can be treated as a Markov state. Let's see a simple example of character bigrams where each character is a Markov state.

# N-Grams:
N gram capture the context in which words are used togeather.For example consider bigrams like "New Yark" insted of breaking it into individual words like "New" and "Yark"

An N-gram is a sequence of N-words in a sentence . Here, N is an integer which stands for number of words in the sequence.

For example, if we put N=1, then it is referred to as a uni-gram. If you put N=2, then it is a bi-gram. If we substitute N=3, then it is a tri-gram.

The bag of words does not take into consideration the order of the words in which they appear in a document, and only individual words are counted.

In some cases, the order of the words might be important.
N-grams captures the context in which the words are used together. For example, it might be a good idea to consider bigrams like “New York” instead of breaking it into individual words like “New” and “York”

Consider the sentence “I like dancing in the rain”

See the Uni-Gram, Bi-Gram, and Tri-Gram cases below.

UNIGRAM: ‘I’, ‘like’, ‘dancing’, ‘in’, ‘the’, ‘rain’

BIGRAM: ‘I like’, ‘like dancing’, ‘dancing in’, ‘in the’, ‘the rain’

TRIGRAM: ‘I like dancing’, ‘like dancing in’, ‘dancing in the’, ‘in the rain’

# Term Frequency, Inverse Document Frequency(TF-IDF):
This is the most popular way to represent documents as feature vectors.TF_IDF stands for Term Frequency Inverse document frequency.
This is the most popular way to represent documents as feature vectors. TF-IDF stands for Term Frequency, Inverse Document Frequency.

TF-IDF measures how important a particular word is with respect to a document and the entire corpus.

Term Frequency:

Term frequency is the measure of the counts of each word in a document out of all the words in the same document. 

TF(w) = (number of times word w appears in a document) / (total number of words in the document)

For example, if we want to find the TF of the word cat which occurs 50 times in a document of 1000 words, then 

TF(cat) = 50 / 1000 = 0.05

Inverse Document Frequency:

IDF is a measure of the importance of a word, taking into consideration the frequency of the word throughout the corpus.

It measures how important a word is for the corpus.

IDF(w) = log(total number of documents / number of documents with w in it)

For example, if the word cat occurs in 100 documents out of 3000, then the IDF is calculated as

IDF(cat) = log(3000 / 100) = 1.47

Finally, to calculate TF-IDF, we multiply these two factors – TF and IDF.

TF-IDF(w) = TF(w) x IDF(w)

TF-IDF(cat) = 0.05 * 1.47 = 0.073

Let’s do some coding.

We’ll use the TfidfVectorizer from scikit-learn for vectorizing the documents.

# SUMMARY:
We first discussed bag of words which is a simple method to convert raw text to numerical vectors. BoW model captures the frequencies of the word occurrences in a text corpus.

However, the drawback with this model is that it is very sparse, and it does not take into consideration the order of words in which they appear in the document.

We then discussed N-grams, which is a sequence of N-words in a document. N-gram tends to capture the context in which the words are used together.

The problem with N-gram is that even here, the resulting matrix is extremely sparse.

Finally, we discussed TF-IDF which measures how important a particular word is with respect to a document and the entire corpus. Words which are rare in a document will have a high score in the TF-IDF vector.







